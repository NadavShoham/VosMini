{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport random",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def euclidean_distance(point1, point2):\n    return np.linalg.norm(point1 - point2)\n\n\ndef calc_avg(Ci, d):\n    sum1 = np.zeros((1, d))\n    size = len(Ci)\n\n    for x in Ci:\n        sum1 += x\n\n    for i in range(d):\n        sum1[0][i] = sum1[0][i] / size\n\n    return sum1\n\n\n# generate 2D data of N points using sklearn\ndef generate_data(N=1000):\n    X, y = make_blobs(n_samples=N, centers=3, n_features=2, random_state=1)\n    plt.scatter(X[:, 0], X[:, 1])\n    plt.title(\"1000 points in 2D\")\n    plt.show()\n    return X\n\n\ndef run_Kmeans(k):\n    X = generate_data()\n    kmeans = Kmeans(k)\n    labels = kmeans.fit(X)\n    plt.scatter(X[:, 0], X[:, 1], c=labels)\n    plt.show()\n\n\ndef run_pdc_dp_means(l):\n    X = generate_data()\n    pdc_dp_means = PDCDPmeans(l)\n    labels = pdc_dp_means.fit(X)\n    plt.scatter(X[:, 0], X[:, 1], c=labels)\n    plt.show()",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "    \"\"\"\n    :param X: numpy array of size (m, d) containing the test samples\n    :param k: the number of clusters\n    :param t: the number of iterations to run\n    :return: a column vector of length m, where C(i) ∈ {1, . . . , k} is the identity of\n    the cluster in which x_i has been assigned.\n    \"\"\"\n\n\nclass Kmeans:\n    def __init__(self, k, max_iter=100):\n        self.k = k\n        self.max_iter = max_iter\n        self.centroids = None\n        self.clusters = None\n        self.assignments = None\n        self.m = None\n        self.d = None\n\n    def fit(self, X):\n        \"\"\"Train the K-Means algorithm on the data X\"\"\"\n        self.m, self.d = X.shape\n        self.centroids = self._init_centroids(X)\n        self.assignments = [-1 for _ in range(self.m)]\n\n        for i in range(self.max_iter):\n            self.clusters = self._create_clusters(X)\n            old_centroids = self.centroids\n            self.centroids = self._calculate_centroids(X)\n            if self._is_converged(old_centroids):\n                break\n        return self.assignments\n\n    def _init_centroids(self, X):\n        \"\"\"Initialize the centroids as k random samples from X\"\"\"\n        return [X[random.randint(0, self.m - 1)] for _ in range(self.k)]\n\n    def _create_clusters(self, X):\n        \"\"\"Assign the samples to the closest centroids to create clusters\"\"\"\n        clusters = [[] for _ in range(self.k)]\n        for idx, sample in enumerate(X):\n            centroid_idx = self._closest_centroid(sample)\n            self.assignments[idx] = centroid_idx\n            clusters[centroid_idx].append(sample)\n        return clusters\n\n    def _closest_centroid(self, sample):\n        \"\"\"Return the index of the closest centroid to the sample\"\"\"\n        distances = [euclidean_distance(sample, point) for point in self.centroids]\n        closest_idx = np.argmin(distances)\n        return closest_idx\n\n    def _calculate_centroids(self, X):\n        \"\"\"Calculate the new centroids as the means of the samples in each cluster, if the cluster is empty,\n        choose a random sample from X\"\"\"\n        return [np.mean(cluster, axis=0) if len(cluster) != 0 else X[np.random.randint(self.m - 1)] for cluster in\n                self.clusters]\n\n    def _is_converged(self, old_centroids):\n        \"\"\"Check if the centroids have changed\"\"\"\n        return np.array_equal(old_centroids, self.centroids)\n",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "   \"\"\"\n    :param X: numpy array of size (m, d) containing the test samples\n    :param l: hyperparameter\n    :param t: the number of iterations to run\n    :return: a column vector of length m, where C(i) ∈ {1, . . . , k} is the identity of\n    the cluster in which x_i has been assigned.\n    \"\"\"\n\n\nclass PDCDPmeans(Kmeans):\n    def __init__(self, l, max_iter=100):\n        super().__init__(k=None, max_iter=max_iter)\n        self.l = l\n        self.j_max = None\n        self.d_max = None\n\n    def fit(self, X):\n        \"\"\"Train the K-Means algorithm on the data X\"\"\"\n        self.m, self.d = X.shape\n\n        self.k = 1\n        self.centroids = self._init_centroid(X)\n        self.assignments = [0 for _ in range(self.m)]\n\n        for i in range(self.max_iter):\n            self.j_max, self.d_max = self.farthest_point(X)\n            if not self.split_if_needed(X):\n                break\n            self.clusters = self._create_clusters(X)\n            old_centroids = self.centroids\n            self.centroids = self._calculate_centroids(X)\n            if self._is_converged(old_centroids):\n                break\n        return self.assignments\n\n    def _init_centroid(self, X):\n        \"\"\"Initialize the centroid as mean of samples from X\"\"\"\n        return [np.mean(X, axis=0)]\n\n    def farthest_point(self, X):\n        \"\"\"Return the index, distance of the farthest point from its corresponding centroid\"\"\"\n        distances = [euclidean_distance(x, self.centroids[self.assignments[idx]]) for idx, x in enumerate(X)]\n        farthest_idx = np.argmax(distances)\n        return farthest_idx, distances[farthest_idx]\n\n    def split_if_needed(self, X):\n        \"\"\"Split the cluster if the farthest point is farther than l\"\"\"\n        if self.d_max > self.l:\n            self.k += 1\n            self.centroids.append(X[self.j_max])\n            self.assignments[self.j_max] = self.k\n            return True\n        return False\n",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def main():\n    run_Kmeans(k=3)\n    run_Kmeans(k=6)\n    run_pdc_dp_means(l=4)\n    run_pdc_dp_means(l=6)\n\n\nif __name__ == \"__main__\":\n    main()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def kmeans(X, k, t=100):\n    \"\"\"\n    :param X: numpy array of size (m, d) containing the test samples\n    :param k: the number of clusters\n    :param t: the number of iterations to run\n    :return: a column vector of length m, where C(i) ∈ {1, . . . , k} is the identity of\n    the cluster in which x_i has been assigned.\n    \"\"\"\n    m = X.shape[0]\n    d = X.shape[1]\n\n    # Initialize cluster centroids randomly\n    cluster_centroids = [X[random.randint(0, m - 1)] for _ in range(k)]\n    cluster_assignments = [-1 for _ in range(m)]\n\n    for _ in range(t):\n        # Assign each point to the closest cluster centroid\n        cluster_assignments = []\n        for x in X:\n            distances = [euclidean_distance(x, centroid) for centroid in cluster_centroids]\n            # Choose the centroid that minimize the distance from x - Define Ci\n            cluster_assignments.append(np.argmin(distances))\n\n        # Recompute the centroids of the clusters\n        new_cluster_centroids = []\n        for i in range(k):\n            Ci = [X[j] for j in range(m) if cluster_assignments[j] == i]  # Build cluster Ci\n\n            if len(Ci) != 0:  # Cluster is not empty\n                avg = calc_avg(Ci, d)\n                new_cluster_centroids.append(avg)  # avg of samples in the cluster\n\n            else:  # If a cluster is empty, initialize its centroid randomly\n                new_cluster_centroids.append(X[np.random.randint(m - 1)])\n\n        # Check if the clusters have changed\n        if np.array_equal(new_cluster_centroids, cluster_centroids):\n            break\n\n        else:\n            cluster_centroids = new_cluster_centroids\n\n    cluster_assignments = np.asarray(cluster_assignments)\n    cluster_assignments = cluster_assignments.reshape((m, 1))\n\n    return cluster_assignments",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def pdc_dp_means(X, l, t=100):\n    \"\"\"\n    :param X: numpy array of size (m, d) containing the test samples\n    :param l: hyperparameter\n    :param t: the number of iterations to run\n    :return: a column vector of length m, where C(i) ∈ {1, . . . , k} is the identity of\n    the cluster in which x_i has been assigned.\n    \"\"\"\n    m = X.shape[0]\n    d = X.shape[1]\n    k = 1\n\n    # Initialize cluster centroids randomly\n    cluster_centroids = [calc_avg(X, d)]\n    cluster_assignments = [1 for _ in range(m)]\n\n    for _ in range(t):\n        # Assign each point to the closest cluster centroid\n        cluster_assignments = [np.argmin([euclidean_distance(x, centroid) for centroid in cluster_centroids]) for x in\n                               X]\n        distances = [euclidean_distance(X[i], cluster_centroids[cluster_assignments[i]]) for i in range(m)]\n        j_max = np.argmax(distances)\n\n        try:\n            j_max = j_max[0]\n        except IndexError:\n            j_max = int(j_max)\n\n        d_max = distances[j_max]\n\n        if d_max > l:\n            k += 1\n            cluster_centroids.append(X[j_max])\n            cluster_assignments[j_max] = k\n        else:\n            break\n\n        # Recompute the centroids of the clusters\n        new_cluster_centroids = []\n        for i in range(k):\n            Ci = [X[j] for j in range(m) if cluster_assignments[j] == i]  # Build cluster Ci\n\n            if len(Ci) != 0:  # Cluster is not empty\n                avg = calc_avg(Ci, d)\n                new_cluster_centroids.append(avg)  # avg of samples in the cluster\n\n            else:  # If a cluster is empty, initialize its centroid randomly\n                new_cluster_centroids.append(X[np.random.randint(m - 1)])\n\n        # Check if the clusters have changed\n        if np.array_equal(new_cluster_centroids, cluster_centroids):\n            break\n\n        else:\n            cluster_centroids = new_cluster_centroids\n\n    cluster_assignments = np.asarray(cluster_assignments)\n    cluster_assignments = cluster_assignments.reshape((m, 1))\n\n    return cluster_assignments\n",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}